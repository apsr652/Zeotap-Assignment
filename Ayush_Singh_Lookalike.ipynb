{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import itertools\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "\n",
        "# Load datasets\n",
        "customers = pd.read_csv(\"Customers.csv\")\n",
        "transactions = pd.read_csv(\"Transactions.csv\")\n",
        "\n",
        "# Dataset Merge\n",
        "customer_transactions = pd.merge(transactions, customers, on=\"CustomerID\", how=\"inner\")\n",
        "\n",
        "# Feature engineering:\n",
        "customer_features = customer_transactions.groupby(\"CustomerID\").agg({\n",
        "    \"Quantity\": \"sum\",  # Total quantity purchased\n",
        "    \"TotalValue\": \"sum\",  # Total transaction value\n",
        "    \"ProductID\": \"nunique\"  # Number of unique products purchased\n",
        "}).reset_index()\n",
        "\n",
        "# Add customer profile features\n",
        "customer_profiles = customers.set_index(\"CustomerID\").drop(columns=[\"CustomerName\"])\n",
        "customer_features = customer_features.merge(customer_profiles, on=\"CustomerID\", how=\"left\")\n",
        "\n",
        "# Convert SignupDate to numeric (days since signup)\n",
        "customer_features[\"SignupDate\"] = pd.to_datetime(customer_features[\"SignupDate\"])\n",
        "customer_features[\"DaysSinceSignup\"] = (pd.Timestamp.now() - customer_features[\"SignupDate\"]).dt.days\n",
        "customer_features = customer_features.drop(columns=[\"SignupDate\"])\n",
        "\n",
        "# Encode categorical columns\n",
        "le_region = LabelEncoder()\n",
        "customer_features[\"Region\"] = le_region.fit_transform(customer_features[\"Region\"])\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "scaled_features = scaler.fit_transform(customer_features.drop(columns=[\"CustomerID\"]))\n",
        "\n",
        "# Create pairwise customer dataset\n",
        "customer_ids = customer_features[\"CustomerID\"].tolist()\n",
        "pairs = list(itertools.combinations(customer_ids, 2))\n",
        "pairwise_data = []\n",
        "\n",
        "for c1, c2 in pairs:\n",
        "    vec1 = customer_features[customer_features[\"CustomerID\"] == c1].drop(columns=\"CustomerID\").values.flatten()\n",
        "    vec2 = customer_features[customer_features[\"CustomerID\"] == c2].drop(columns=\"CustomerID\").values.flatten()\n",
        "    feature_diff = abs(vec1 - vec2)  # Feature difference between two customers using vectors\n",
        "    pairwise_data.append({\n",
        "        \"Customer1\": c1,\n",
        "        \"Customer2\": c2,\n",
        "        \"Similarity\": 1 if abs(vec1.sum() - vec2.sum()) < 0.1 * vec1.sum() else 0,  # Similar if differences are small\n",
        "        **{f\"FeatureDiff_{i}\": diff for i, diff in enumerate(feature_diff)}\n",
        "    })\n",
        "\n",
        "pairwise_df = pd.DataFrame(pairwise_data)\n",
        "\n",
        "# Split features and target\n",
        "X = pairwise_df.drop(columns=[\"Customer1\", \"Customer2\", \"Similarity\"])\n",
        "y = pairwise_df[\"Similarity\"]\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Random Forest Classifier\n",
        "rf_model = RandomForestClassifier(random_state=42, n_estimators=100)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "recommendations = {}\n",
        "\n",
        "for customer_id in customer_ids[:20]:\n",
        "    similarities = []\n",
        "    for other_id in customer_ids:\n",
        "        if other_id != customer_id:\n",
        "            vec1 = customer_features[customer_features[\"CustomerID\"] == customer_id].drop(columns=\"CustomerID\").values.flatten()\n",
        "            vec2 = customer_features[customer_features[\"CustomerID\"] == other_id].drop(columns=\"CustomerID\").values.flatten()\n",
        "            feature_diff = abs(vec1 - vec2).reshape(1, -1)\n",
        "            # Create a DataFrame for feature_diff with matching column names\n",
        "            feature_diff_df = pd.DataFrame(feature_diff, columns=X.columns)\n",
        "            # Probability of being similar\n",
        "            similarity_score = rf_model.predict_proba(feature_diff_df)[0][1]\n",
        "            similarities.append((other_id, round(similarity_score, 4)))\n",
        "\n",
        "    # Sort by similarity score and select the top 3\n",
        "    top_3 = sorted(similarities, key=lambda x: x[1], reverse=True)[:3]\n",
        "    recommendations[customer_id] = top_3\n",
        "\n",
        "# Save recommendations to CSV\n",
        "recommendations_df = pd.DataFrame({\n",
        "    \"CustomerID\": list(recommendations.keys()),\n",
        "    \"Lookalikes\": [str(recommendations[customer]) for customer in recommendations]\n",
        "})\n",
        "\n",
        "recommendations_df.to_csv(\"Lookalike.csv\", index=False)\n",
        "\n",
        "print(\"Lookalike recommendations CSV generated!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70I4lkPve4IV",
        "outputId": "8557e615-094b-480b-a667-f880384bcf6f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lookalike recommendations CSV generated!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JbfcD8QDe8Z1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}